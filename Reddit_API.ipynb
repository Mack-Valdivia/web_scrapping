{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32edfc9",
   "metadata": {},
   "source": [
    "# Ejercicio 2 — API de Reddit (PRAW) y Análisis de Sentimientos (sin .env)\n",
    "\n",
    "**Objetivo:** \n",
    "- Conectar a la API de Reddit (PRAW) usando credenciales cargadas directamente en variables de entorno (sin archivo `.env`).\n",
    "- Recopilar publicaciones de subreddits políticos y extraer campos clave.\n",
    "- Seleccionar publicaciones más relevantes y extraer comentarios.\n",
    "- Guardar resultados en CSV vinculando comentarios con su post padre.\n",
    "- (Opcional) Analizar sentimiento de comentarios con VADER (NLTK) y extraer tokens frecuentes de títulos.\n",
    "\n",
    "**Subreddits:** `r/politics`, `r/PoliticalDiscussion`, `r/worldnews`  \n",
    "**Criterios principales:** 20 publicaciones por subreddit (hot o top) y 5 comentarios por cada post relevante.  \n",
    "**Filtro opcional:** Últimos 6 meses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1c3bb",
   "metadata": {},
   "source": [
    "## Parte 1: Configuración de la API de Reddit y recopilación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c02571",
   "metadata": {},
   "source": [
    "## 1) Requisitos\n",
    "\n",
    "Ejecutar (una sola vez) para instalar dependencias en su entorno:\n",
    "\n",
    "```bash\n",
    "pip install praw pandas nltk\n",
    "```\n",
    "> Nota: Aquí **no** usamos `python-dotenv` porque las credenciales se definen directamente en variables de entorno dentro del propio notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ddef3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credenciales cargadas en variables de entorno (os.environ).\n"
     ]
    }
   ],
   "source": [
    "# --- Credenciales directamente en variables de entorno (sin .env) ---\n",
    "import os\n",
    "\n",
    "os.environ[\"REDDIT_CLIENT_ID\"] = \"6J0zagocSCNuuh3rDnKvWA\"\n",
    "os.environ[\"REDDIT_CLIENT_SECRET\"] = \"SyGg3smtxOdrFKGlX9tiHQG2_y8HEw\"\n",
    "os.environ[\"REDDIT_USER_AGENT\"] = \"Python:PoliticalSentimentAnalyzer:v1.0 (by /u/Reddit-MD-170425)\"\n",
    "\n",
    "# Opcional (modo script con login; no es necesario para lectura):\n",
    "os.environ[\"REDDIT_USERNAME\"] = \"Reddit-MD-170425\"\n",
    "os.environ[\"REDDIT_PASSWORD\"] = \"MD-17-123\"\n",
    "\n",
    "print(\"Credenciales cargadas en variables de entorno (os.environ).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c01a28b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de entorno detectadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "# %% Imports y carga de credenciales desde os.environ\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Iterable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import praw\n",
    "from praw.models import Comment\n",
    "from prawcore.exceptions import RequestException, ResponseException, OAuthException, Forbidden\n",
    "\n",
    "# Opcional: NLTK VADER para sentimiento\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Leer variables de entorno (de la celda anterior)\n",
    "CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "USERNAME = os.getenv(\"REDDIT_USERNAME\")\n",
    "PASSWORD = os.getenv(\"REDDIT_PASSWORD\")\n",
    "USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\", \"Python:PoliticalSentimentAnalyzer:v1.0 (by /u/unknown)\")\n",
    "\n",
    "missing = [k for k, v in {\n",
    "    \"REDDIT_CLIENT_ID\": CLIENT_ID,\n",
    "    \"REDDIT_CLIENT_SECRET\": CLIENT_SECRET,\n",
    "    \"REDDIT_USER_AGENT\": USER_AGENT,\n",
    "}.items() if not v]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltan variables en entorno: {missing}\")\n",
    "else:\n",
    "    print(\"Variables de entorno detectadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee7ad5",
   "metadata": {},
   "source": [
    "## Parte 2: Recopilación y almacenamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af6e30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Parámetros del ejercicio\n",
    "TARGET_SUBS = [\"politics\", \"PoliticalDiscussion\", \"worldnews\"]  # nombres canónicos de subreddits\n",
    "POSTS_PER_SUB = 20             # 20 publicaciones por subreddit\n",
    "SORT = \"hot\"                   # \"hot\" o \"top\"\n",
    "ONLY_LAST_6_MONTHS = False     # True para filtrar solo últimos 6 meses\n",
    "N_RELEVANT_POSTS = 10          # subconjunto de posts más relevantes (por score)\n",
    "COMMENTS_PER_POST = 5          # 5 comentarios por post\n",
    "\n",
    "OUTPUT_POSTS_CSV = \"reddit_posts.csv\"\n",
    "OUTPUT_COMMENTS_CSV = \"reddit_comments.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "932f144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Conexión PRAW y utilidades\n",
    "def init_reddit() -> praw.Reddit:\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=CLIENT_ID,\n",
    "            client_secret=CLIENT_SECRET,\n",
    "            username=USERNAME,\n",
    "            password=PASSWORD,\n",
    "            user_agent=USER_AGENT,\n",
    "            ratelimit_seconds=5,\n",
    "        )\n",
    "        _ = reddit.read_only  # verificación simple\n",
    "        return reddit\n",
    "    except (OAuthException, ResponseException) as e:\n",
    "        raise RuntimeError(f\"Error autenticando con Reddit: {e}\")\n",
    "\n",
    "def six_months_ago_ts() -> float:\n",
    "    return time.time() - 180 * 24 * 3600  # ~6 meses\n",
    "\n",
    "def is_within_last_6_months(created_utc: float) -> bool:\n",
    "    return created_utc >= six_months_ago_ts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4963947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Estructuras de datos\n",
    "@dataclass\n",
    "class PostRecord:\n",
    "    subreddit: str\n",
    "    post_id: str\n",
    "    title: str\n",
    "    score: int\n",
    "    num_comments: int\n",
    "    url: str\n",
    "    permalink: str\n",
    "    created_utc: float\n",
    "\n",
    "@dataclass\n",
    "class CommentRecord:\n",
    "    post_id: str\n",
    "    comment_id: str\n",
    "    body: str\n",
    "    score: int\n",
    "    created_utc: float\n",
    "    subreddit: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "455f521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Descarga de posts\n",
    "def fetch_posts_for_subreddit(\n",
    "    reddit: praw.Reddit,\n",
    "    sub_name: str,\n",
    "    limit: int = 20,\n",
    "    sort: str = \"hot\",\n",
    "    only_last_6_months: bool = False,\n",
    ") -> List[PostRecord]:\n",
    "    sub = reddit.subreddit(sub_name)\n",
    "    gen = sub.top(time_filter=\"year\") if sort == \"top\" else sub.hot()\n",
    "\n",
    "    results: List[PostRecord] = []\n",
    "    for submission in gen:\n",
    "        if only_last_6_months and not is_within_last_6_months(submission.created_utc):\n",
    "            continue\n",
    "        pr = PostRecord(\n",
    "            subreddit=sub_name,\n",
    "            post_id=submission.id,\n",
    "            title=submission.title or \"\",\n",
    "            score=int(submission.score or 0),\n",
    "            num_comments=int(submission.num_comments or 0),\n",
    "            url=submission.url or \"\",\n",
    "            permalink=f\"https://www.reddit.com{submission.permalink}\",\n",
    "            created_utc=float(submission.created_utc or 0.0),\n",
    "        )\n",
    "        results.append(pr)\n",
    "        if len(results) >= limit:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "def fetch_posts(\n",
    "    reddit: praw.Reddit,\n",
    "    sub_names: Iterable[str],\n",
    "    posts_per_sub: int,\n",
    "    sort: str = \"hot\",\n",
    "    only_last_6_months: bool = False,\n",
    ") -> List[PostRecord]:\n",
    "    all_posts: List[PostRecord] = []\n",
    "    for name in sub_names:\n",
    "        try:\n",
    "            posts = fetch_posts_for_subreddit(\n",
    "                reddit, name, posts_per_sub, sort, only_last_6_months\n",
    "            )\n",
    "            all_posts.extend(posts)\n",
    "        except Forbidden:\n",
    "            print(f\"[ADVERTENCIA] Acceso prohibido a r/{name}. Omitiendo...\")\n",
    "        except RequestException as e:\n",
    "            print(f\"[ADVERTENCIA] Error de red en r/{name}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ADVERTENCIA] Error inesperado en r/{name}: {e}\")\n",
    "    return all_posts\n",
    "\n",
    "def select_most_relevant_posts(posts: List[PostRecord], n: int = 10) -> List[PostRecord]:\n",
    "    posts_sorted = sorted(posts, key=lambda p: p.score, reverse=True)\n",
    "    return posts_sorted[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afb9f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Descarga de comentarios\n",
    "def fetch_top_comments_for_post(\n",
    "    reddit: praw.Reddit,\n",
    "    post_id: str,\n",
    "    subreddit: str,\n",
    "    max_comments: int = 5,\n",
    ") -> List[CommentRecord]:\n",
    "    submission = reddit.submission(id=post_id)\n",
    "    submission.comment_sort = \"top\"\n",
    "    submission.comments.replace_more(limit=0)\n",
    "\n",
    "    top_level_comments = [c for c in submission.comments if isinstance(c, Comment)]\n",
    "    top_level_comments.sort(key=lambda c: getattr(c, \"score\", 0), reverse=True)\n",
    "\n",
    "    comments: List[CommentRecord] = []\n",
    "    for c in top_level_comments[:max_comments]:\n",
    "        body = getattr(c, \"body\", \"\")\n",
    "        score = int(getattr(c, \"score\", 0) or 0)\n",
    "        created = float(getattr(c, \"created_utc\", 0.0) or 0.0)\n",
    "        cr = CommentRecord(\n",
    "            post_id=post_id,\n",
    "            comment_id=c.id,\n",
    "            body=body,\n",
    "            score=score,\n",
    "            created_utc=created,\n",
    "            subreddit=subreddit,\n",
    "        )\n",
    "        comments.append(cr)\n",
    "    return comments\n",
    "\n",
    "def fetch_comments_for_posts(\n",
    "    reddit: praw.Reddit,\n",
    "    posts: List[PostRecord],\n",
    "    comments_per_post: int = 5,\n",
    ") -> List[CommentRecord]:\n",
    "    all_comments: List[CommentRecord] = []\n",
    "    for p in posts:\n",
    "        try:\n",
    "            comms = fetch_top_comments_for_post(reddit, p.post_id, p.subreddit, comments_per_post)\n",
    "            all_comments.extend(comms)\n",
    "        except Forbidden:\n",
    "            print(f\"[ADVERTENCIA] Comentarios restringidos en post {p.post_id}.\")\n",
    "        except RequestException as e:\n",
    "            print(f\"[ADVERTENCIA] Error de red en post {p.post_id}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ADVERTENCIA] Error inesperado en post {p.post_id}: {e}\")\n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a55d99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% VADER (opcional) y tokens de títulos\n",
    "def ensure_vader():\n",
    "    try:\n",
    "        nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"vader_lexicon\")\n",
    "\n",
    "def add_sentiment_scores(df_comments: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_comments.empty:\n",
    "        return df_comments\n",
    "    ensure_vader()\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df_comments = df_comments.copy()\n",
    "    df_comments[\"sentiment_compound\"] = df_comments[\"body\"].fillna(\"\").map(\n",
    "        lambda t: sia.polarity_scores(t)[\"compound\"]\n",
    "    )\n",
    "    return df_comments\n",
    "\n",
    "def top_title_tokens(posts_df: pd.DataFrame, k: int = 20) -> pd.DataFrame:\n",
    "    if posts_df.empty:\n",
    "        return posts_df\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    stop_basic = {\n",
    "        # inglés\n",
    "        \"the\",\"a\",\"an\",\"to\",\"of\",\"in\",\"on\",\"for\",\"and\",\"or\",\"is\",\"are\",\"was\",\"were\",\n",
    "        \"with\",\"at\",\"by\",\"from\",\"it\",\"this\",\"that\",\"as\",\"be\",\"has\",\"have\",\"had\",\n",
    "        # español\n",
    "        \"el\",\"la\",\"los\",\"las\",\"de\",\"del\",\"un\",\"una\",\"unos\",\"unas\",\"y\",\"o\",\"en\",\n",
    "        \"para\",\"por\",\"con\",\"es\",\"son\",\"fue\",\"fueron\",\"ha\",\"han\",\"haber\",\"se\",\"lo\",\n",
    "        \"al\",\"como\",\"más\",\"mas\",\"menos\",\"sin\",\"sobre\",\"ya\",\"no\",\"si\"\n",
    "    }\n",
    "    tokens = []\n",
    "    for t in posts_df[\"title\"].dropna().astype(str).tolist():\n",
    "        t = t.lower()\n",
    "        t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", t)\n",
    "        t = re.sub(r\"[^\\wáéíóúñüäëïöü]+\", \" \", t)\n",
    "        parts = [w for w in t.split() if len(w) > 2 and w not in stop_basic]\n",
    "        tokens.extend(parts)\n",
    "    from collections import Counter\n",
    "    cnt = Counter(tokens)\n",
    "    common = cnt.most_common(k)\n",
    "    return pd.DataFrame(common, columns=[\"token\", \"freq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeceeaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando 20 posts por subreddit (SORT=hot, ult_6m=False)...\n",
      "Total posts: 60\n",
      "Posts relevantes para comentarios: 10\n"
     ]
    }
   ],
   "source": [
    "# %% Ejecución principal\n",
    "def main():\n",
    "    reddit = init_reddit()\n",
    "\n",
    "    print(f\"Descargando {POSTS_PER_SUB} posts por subreddit (SORT={SORT}, ult_6m={ONLY_LAST_6_MONTHS})...\")\n",
    "    posts = fetch_posts(\n",
    "        reddit,\n",
    "        TARGET_SUBS,\n",
    "        posts_per_sub=POSTS_PER_SUB,\n",
    "        sort=SORT,\n",
    "        only_last_6_months=ONLY_LAST_6_MONTHS,\n",
    "    )\n",
    "    posts_df = pd.DataFrame([asdict(p) for p in posts])\n",
    "    print(f\"Total posts: {len(posts_df)}\")\n",
    "\n",
    "    relevant = select_most_relevant_posts(posts, N_RELEVANT_POSTS)\n",
    "    print(f\"Posts relevantes para comentarios: {len(relevant)}\")\n",
    "\n",
    "    comments = fetch_comments_for_posts(reddit, relevant, comments_per_post=COMMENTS_PER_POST)\n",
    "    comments_df = pd.DataFrame([asdict(c) for c in comments])\n",
    "    print(f\"Total comentarios: {len(comments_df)}\")\n",
    "\n",
    "    # (Opcional) Sentimiento\n",
    "    comments_df = add_sentiment_scores(comments_df)\n",
    "\n",
    "    # Guardado\n",
    "    posts_df.to_csv(OUTPUT_POSTS_CSV, index=False, encoding=\"utf-8\")\n",
    "    comments_df.to_csv(OUTPUT_COMMENTS_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Guardado: {OUTPUT_POSTS_CSV} | {OUTPUT_COMMENTS_CSV}\")\n",
    "\n",
    "    # Vista rápida\n",
    "    display(posts_df.head(10))\n",
    "    display(comments_df.head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6b51d",
   "metadata": {},
   "source": [
    "## Notas finales\n",
    "- Este notebook **no** usa `.env`. Las credenciales se cargan en variables de entorno mediante una celda al inicio.\n",
    "- Si algún subreddit está restringido, se mostrará una advertencia y el proceso continuará con los demás.\n",
    "- Para usar **solo los últimos 6 meses**, establecer `ONLY_LAST_6_MONTHS = True` y, de preferencia, `SORT = \"top\"`.\n",
    "- Los CSV se guardan en la misma carpeta del notebook y se relacionan por `post_id`.\n",
    "- Mantenga sus credenciales en privado si comparte este notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
